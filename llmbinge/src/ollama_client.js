// This file handles the calls to ollama API

// TODO: This should be configurable
let ollama_url = "http://localhost:11434/api/generate"
let model = "mistral"


// set configuration
export function set_config(cfg_url, cfg_model) {
    ollama_url = cfg_url
    model = cfg_model
}

// Ollama has a streaming API. This helps to get incremental output for the
// text generation. This function handles the post part.
function post_stream(url, data) {
    let post_params = {
        method: "POST",
        headers: {
            "Content-Type": "application/json"
        },
        body: JSON.stringify(data)
    }
    return fetch(url, post_params)
}

// Handle ollama call
//
// This is the core function which handles the calls to ollama API
// text: query string
// push_text: a callback function to handle incremental updates
//
// Returns: a future with the full text
export async function llm_generate(text, push_text=null) {
    const data = {
        prompt: text,
        model: model,
        stream: true,
        options: {
            seed: Math.floor(Math.random() * 10000)
        }
    }
    const response = await post_stream(ollama_url, data)
    const reader = response.body.getReader()
    let chunks = []
    let done = false
    while (true) {
        let chunk = await reader.read()
        done = chunk.done
        if (done) {
            break
        }
        if (!chunk.value) {
            continue
        }
        let decoder = new TextDecoder("utf-8")
        let data = decoder.decode(chunk.value)
        let obj = null
        try {
            obj = JSON.parse(data)
        } catch(e) {
            console.log("ERROR: json parse error")
            continue
        }
        if ((obj != null) && (obj.response != null)) {
            chunks.push(obj.response)
            if (push_text != null) {
                push_text(obj.response)
            }
        }
    }
    return chunks.join("")
}

// Splits the given string data into lines.
function get_all_lines(data) {
    let parts = data.split("\n")
    let remaining = ""
    if (data.endsWith("\n")) {
        // Split adds an empty string. Remove that.
        parts.pop()
    } else {
        // Last one is not a full line since there is no following \n.
        // It should go with the "remaining"
        remaining = parts.pop()
    }
    return [parts, remaining]
}

// Produces a list of related topics based on the query and its response 
// generated by LLM.
//
// query: query used
// response: response generated by the LLM
// inc_update: callback to handle updates
export async function get_related(query, response, inc_update) {
    const related_query = `
    Based on the following query and response, give suggestions for
    topics to explore further.

    Query: ${query}

    Response: ${response}

    Only produce 10 most relevant topics in the following output format.
    It should be a bullet list of topics. Each topic should be short.
    -- output format start --
    * topic1 (just a name or a phrase, max 8 words)
    * topic2 (just a name or a phrase, max 8 words)
    * so on
    -- output format end --
    `
    // number of useful output lines produced
    let lines_added = 0
    // the state that keeps track of the "remaining"
    let rem = ""
    // whole thing produced by the LLM
    let full_text = ""

    // send updates to the caller using inc_update()
    let send_update = (line) => {
        // lines may start with spaces or tabs
        line = line.trim()
        // Avoid lines that are not bulletpoints
        // some models are creative and use • instead of *
        if (!line.startsWith("*") && !line.startsWith("•")) {
            return
        }
        // Remove the bullet
        line = line.substr(1).trim()
        // Send big enough lines to the caller via inc_update
        if (line.length > 3) {
            inc_update(line)
            lines_added++
        }
    }

    // this parts generates the result
    await llm_generate(related_query, (text) => {
        rem += text
        full_text += text
        // get the lines if we have some
        let [splits, remaining] = get_all_lines(rem)
        for (let idx = 0; idx < splits.length; idx++) {
            let part = splits[idx]
            send_update(part)
        }
        rem = remaining
    })
    send_update(rem)
    // Nasty hack
    // Sometimes we don't get output in the format we want. So try again.
    if (lines_added == 0) {
        console.log("WARNING: No lines found!")
        console.log(full_text)
        await get_related(query, response, inc_update)
    }
}


function pick_line_with_prefix(response, prefix) {
    let lines = response.split("\n")
    let result = ""
    let prefix_lower = prefix.toLowerCase()
    for (let idx = 0; idx < lines.length; idx++) {
        let line = lines[idx].trim()
        if (line.toLowerCase().startsWith(prefix_lower)) {
            result = line.slice(prefix.length).trim()
        }
    }
    return result
}


// Create a new prompt based on the title and the aspect
//
// Part of the aspect feature. We have fixed set of aspects for every response.
// When clicking on an aspect (eg: history), we want to create a new query.
// 
// title: the title of the existing topic
// aspect: the aspect to explore
//
// If the title of the existing node is "what is vitamin-c" and aspect is
// "history", this function should produce as output like:
// "What is the history of vitamin-c?"
export async function llm_get_aspect_query(title, aspect) {
    const prefix = "Query-rephrased:"
    // It was not straightforward to get this prompt working.
    // What worked on one model didn't work on another.
    // Some models are too creative. See #prompt-engineering in Footnotes.
    const aspect_query = `
    Agressively simplify and rephrase the query given below.
    Do not ask for further clarification.
    Do not include extra information other what is in the query.

    Query: In the context of "${title}", elaborate on the aspect "${aspect}"

    Output should be of the following format:
    --- format ---
    Query: same as the above one
    Topic: topic of the query
    Aspect: what is the aspect of discussion
    ${prefix} simplified query
    ---
    `.trim()

    let response = await llm_generate(aspect_query)
    let result = pick_line_with_prefix(response, prefix)
    return result
}

function choice(arr) {
    let index = Math.floor(Math.random() * arr.length)
    return arr[index]
}


export function split_remove_minus(data) {
  return data.split(/\s+/)
             .map(s => s.replace(/-/g, " "))
             .filter(s => s.trim() != "")
             .sort()
}


const fields_of_study = split_remove_minus(`
science engineering medicine biology architecture
electronics computer mathematics programming electrical
history geography arts epics culture food industry
economics humanities politics general-knowledge
manufacturing travel psychology health research
spirituality religion commerce physics chemistry
archeology forensic crime
`)

const specialities = split_remove_minus(`
named-after-a-person old very-old recent
surprising rare person place simple complex
idea concept question standard accepted 
pseudo-science conspiracy well-known
invention discovery theory conjecture funny
mystery unexplained spooky
`)

const prefixes = split_remove_minus(`
Fact: Idea: Topic: Title:
Astonishing-fact:
Random-fact:
Question: 
`)

export async function generate_random_topic() {
    const prefix = choice(prefixes)
    const field = choice(fields_of_study)
    const speciality = choice(specialities)
    const prompt = `
    Give a fact, idea, or topic in or related to the field ${field}.
    It should have the following speciality: ${speciality}.
    Produce output exactly in the following format:
    --- start ---
    field: fill here
    speciality: fill here
    ${prefix} fill (max 6 words)
    --- end ---
    Maximum 40 words.
    `
    let response = await llm_generate(prompt)
    let result = pick_line_with_prefix(response, prefix)
    result = result.replace(/\.$/,"")
    return result
}


// #prompt-engineering
//
// One key issue is that some models are way more creative than what we expect.
// There are several ways to format the output despite what we ask for.
// There are a few key ideas I learned:
// 1. If we ask for x, and model the gives y: ask the model to produce both x and y.
// 2. Expect output to contain more words than what we expect. Some parsing
//    will be required to pick what we need.
// 3. Sometimes it may be required to lead the model to the final result by asking
//    it to produce intermediate steps.
